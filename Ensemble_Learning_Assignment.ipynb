{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Theory & Practical Questions"
      ],
      "metadata": {
        "id": "CoN_mDJxp5WY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1:  What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it.\n",
        "\n",
        "Answer:Ensemble Learning is a technique in machine learning where we combine multiple models to make better predictions. The key idea is that a group of weak or base learners, when combined, can perform stronger than a single model. By reducing errors, variance, or bias, we achieve more accurate and reliable results."
      ],
      "metadata": {
        "id": "6TTD_y_LqBtu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the difference between Bagging and Boosting?\n",
        "\n",
        "Answer:Bagging and Boosting are both ensemble methods but work differently. In Bagging, we train multiple models in parallel on random subsets of data and then combine their results to reduce variance. In Boosting, models are trained sequentially where each new model focuses on correcting the errors of the previous one, which helps reduce bias and improve accuracy."
      ],
      "metadata": {
        "id": "iqVhza3Oq3pm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is bootstrap sampling and what role does it play in Bagging methods\n",
        "like Random Forest?\n",
        "\n",
        "Answer:Bootstrap sampling is a technique where we randomly select samples from the dataset with replacement, meaning some data points may repeat while others may be left out. In Bagging methods like Random Forest, it helps create diverse training sets for each model. This diversity reduces overfitting and makes the overall prediction more stable and accurate."
      ],
      "metadata": {
        "id": "gMqf6pSprB4O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?\n",
        "\n",
        "Answer:Out-of-Bag (OOB) samples are the data points that are not included in a bootstrap sample while training a model. Since each model in Bagging leaves out some data, these unused points can be used to test the model’s performance. The OOB score is calculated from these samples and gives us an unbiased estimate of the ensemble model’s accuracy without needing a separate validation set."
      ],
      "metadata": {
        "id": "zXR3T9mkrFKW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest.\n",
        "\n",
        "Answer:In a single Decision Tree, feature importance is calculated based on how much each feature reduces impurity (like Gini index or entropy) when it makes a split. However, this can sometimes give a biased view. In a Random Forest, feature importance is averaged over many trees, making it more reliable and stable, as it reduces the effect of randomness from a single tree."
      ],
      "metadata": {
        "id": "RfECaq8MrM5-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to:\n",
        "* Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "* Train a Random Forest Classifier\n",
        "* Print the top 5 most important features based on feature importance scores.\n",
        "* (Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "mcqp8udutrVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer:\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "\n",
        "model = RandomForestClassifier(random_state=1)\n",
        "model.fit(X, y)\n",
        "\n",
        "\n",
        "importances = model.feature_importances_\n",
        "feature_importance = pd.Series(importances, index=data.feature_names)\n",
        "\n",
        "\n",
        "print(\"Top 5 Important Features:\")\n",
        "print(feature_importance.sort_values(ascending=False).head(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaE1SPjhq1xT",
        "outputId": "fd67a324-ed05-4847-bf43-a01b18d4928a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Important Features:\n",
            "worst concave points    0.123350\n",
            "worst perimeter         0.115661\n",
            "worst area              0.105248\n",
            "worst radius            0.102798\n",
            "mean concave points     0.100735\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "* Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "* Evaluate its accuracy and compare with a single Decision Tree\n",
        "* (Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "R9Je_MSguenG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer:\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "data = load_iris()\n",
        "\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=1)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_pred = dt.predict(X_test)\n",
        "dt_acc = accuracy_score(y_test, dt_pred)\n",
        "\n",
        "bagging = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=1)\n",
        "bagging.fit(X_train, y_train)\n",
        "bag_pred = bagging.predict(X_test)\n",
        "bag_acc = accuracy_score(y_test, bag_pred)\n",
        "\n",
        "print(\"Decision Tree Accuracy:\", dt_acc)\n",
        "print(\"Bagging Classifier Accuracy:\", bag_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bru4MdbruvCv",
        "outputId": "fd638a05-8f9c-4500-8526-57db70c2606c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 0.9555555555555556\n",
            "Bagging Classifier Accuracy: 0.9555555555555556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "* Train a Random Forest Classifier\n",
        "* Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "* Print the best parameters and final accuracy\n",
        "* (Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "Ji6qPH63vXjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer:\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "data = load_breast_cancer()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\n",
        "rf = RandomForestClassifier(random_state=1)\n",
        "\n",
        "param_grid = {\n",
        "    \"n_estimators\": [50, 100, 150],\n",
        "    \"max_depth\": [None, 5, 10]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(rf, param_grid, cv=5, scoring=\"accuracy\")\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "print(\"Final Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZQQ4Z2SvALZ",
        "outputId": "cc15543f-89ad-4d5b-866c-bd220da1a2a3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'n_estimators': 100}\n",
            "Final Accuracy: 0.9473684210526315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "* Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "* Compare their Mean Squared Errors (MSE)\n",
        "* (Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "NzmbisWfwH-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer:\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "\n",
        "data = fetch_california_housing()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\n",
        "bagging = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=50, random_state=1)\n",
        "bagging.fit(X_train, y_train)\n",
        "bag_pred = bagging.predict(X_test)\n",
        "bag_mse = mean_squared_error(y_test, bag_pred)\n",
        "\n",
        "rf = RandomForestRegressor(n_estimators=50, random_state=1)\n",
        "rf.fit(X_train, y_train)\n",
        "rf_pred = rf.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_pred)\n",
        "\n",
        "print(\"Bagging Regressor MSE:\", bag_mse)\n",
        "print(\"Random Forest Regressor MSE:\", rf_mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IV2S3Cnv1dQ",
        "outputId": "16acbbb8-c20c-4e5f-e4f1-609b926e21d1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE: 0.25787382250585034\n",
            "Random Forest Regressor MSE: 0.25772464361712627\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "* Choose between Bagging or Boosting\n",
        "* Handle overfitting\n",
        "* Select base models\n",
        "* Evaluate performance using cross-validation\n",
        "* Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n",
        "\n",
        "Answer:\n",
        "* Step 1: Choose between Bagging or Boosting\n",
        "We first check if our dataset has high variance or high bias. If the model tends to overfit (high variance), we prefer Bagging like Random Forest. If the model underfits and misses patterns, we use Boosting like XGBoost or AdaBoost to focus on hard-to-predict cases.\n",
        "\n",
        "* Step 2: Handle overfitting\n",
        "We use techniques like limiting tree depth, reducing the number of estimators, and applying regularization in boosting. We also use cross-validation to tune parameters and ensure the model performs well on unseen data.\n",
        "\n",
        "* Step 3: Select base models\n",
        "We usually start with Decision Trees because they are simple and work well in ensembles. For more complex data, we can also try logistic regression or shallow models as base learners depending on the problem.\n",
        "\n",
        "* Step 4: Evaluate performance using cross-validation\n",
        "We split the dataset into multiple folds and train/test on different parts. This gives us an average performance score that is more reliable than a single train-test split. It helps us choose the best ensemble setup.\n",
        "\n",
        "* Step 5: Justify ensemble learning in real-world context\n",
        "In loan default prediction, wrong decisions can be very costly. Ensemble methods combine multiple models, making predictions more stable and accurate. This reduces both false approvals and false rejections, helping the financial institution make safer and smarter lending decisions.\n",
        "\n",
        "* By carefully choosing the right ensemble method, tuning parameters, and validating results, we build a strong predictive model. Ensemble learning reduces errors and provides more reliable insights, which is crucial for making better financial decisions like predicting loan defaults."
      ],
      "metadata": {
        "id": "oiot2DktwwS_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Iv3-7LvrwZ9v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}